---
title: "Turbulence Analysis"
author: "Tingnan Hu, Peter Liu, Islina Shan, Ken Ye"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
#global code chunk settings for knitting
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

# Introduction

Turbulence is one of the fascinating topics in the research in fluid dynamics. It is characterized its chaotic motion, rapid fluctuations and lack of predictable patterns. Yet, there have been numerous attempts in scientific literature trying to model the behavior of turbulent flows, as turbulent flows are prevalent in our world and are the underlying forces that drive plenty of the physical processes, from wisps of smoking swirling up from the cigarette to mixing of chemicals in industrial processes. A better understanding and prediction of turbulent flow will help us gain a deeper insight into a wide range of applications, such as improved aerodynamics in airplane designs and better climatic modelling.

A subdomain in turbulent flow research deals with particle clustering in turbulent flow focusing on small particles' behavior in turbulent fluids. For our project, we are provided with a set of simulation results on small particle probability distribution. The outcome variable was originally a probability distribution for particle cluster volumes, but it was converted into its first four raw moments $E[X]$ to $E[X^4]$ facilitate analysis. The predictor set contains three variables:

-   Reynolds number Re, which provides information on the type of flow a fluid is experiencing. A low Re corresponds with laminar flow (smooth and orderly), while a high Re corresponds with turbulent flow

-   Gravitational acceleration Fr, which measures the gravitational forces particles are experiencing

-   Stokes number St, where larger value corresponds with larger particle size

The main research objective of our project will be to build a viable statistical model to predict the response variable (first four raw moments of particle probability distribution) using the three predictors at hand, utilizing the data in a training set provided. Specifically, we are interested in the following:

-   Does there exist a significant linear relationship between the predictors and the raw four moments?

-   Is there any significant interaction effects between predictors on the response variables?

-   Does a linear regression model suffice? Or a more complex model is needed to better explain the relationship between the predictor and response

-   Are identified effects for predictors the same for all moments, or they differ for each different moment?

Ultimately, we wish our model to capture sufficient trends in our training data, so that we can predict the four moments in our test set data as accurately as possible.

# Methodology

```{r}
library(ggplot2)
library(dplyr)
library(car)
library(caret)
library(knitr)
library(kableExtra)
library(tidyverse)

train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv")
```

We begin by some transformations on both predictor and response variables. For predictor variable, we first noticed that Fr only takes on 0.052, 0.3 and Inf in our training and testing data set, and directly using it is not viable since it contains infinity. Since Fr\<1 corresponds with a subcritical flow while Fr \> 1 corresponds with a super critical flow, we create a new categorical variable `flow` by the following:

```{r}
df <- data.frame(
  Flow = c('super subcritical', 'subcritical', 'supercritical'),
  Fr = c('Fr < 0.1', '0.1 < Fr < 1', 'Fr > 1')
)

table <- kable(df) %>%
    kable_styling("striped", full_width = FALSE) %>%
    column_spec(1, bold = TRUE) %>% 
    kable_styling(latex_options = "HOLD_position", full_width = F)

table
```

```{r}
train <- train %>% 
  mutate(flow = case_when(
    Fr < 0.1 ~ 'super subcritical',
    Fr < 1 & Fr > 0.1 ~ 'subcritical',
    Fr > 1 ~ 'supercritical'
  )) %>% 
  mutate(
    log.moment.2 <- log(R_moment_2),
    log.moment.3 <- log(R_moment_3),
    log.moment.4 <- log(R_moment_4)
  ) %>% 
  mutate(
    Re.fac = case_when(
      Re == 90 ~ 'low',
      Re == 224 ~ 'mid',
      Re == 398 ~ 'high'
    )
  )
```

```{r}
par(mfrow = c(2,2))
ggplot(train, aes(x = R_moment_1)) + geom_histogram()
```

# EDA

```{r}
summary(train)
```

```{r}
# Create histograms for the predictor variables
ggplot(train, aes(x = Re)) + geom_histogram()
ggplot(train, aes(x = Fr)) + geom_histogram()
ggplot(train, aes(x = St)) + geom_histogram()
ggplot(train, aes(x = R_moment_1)) + geom_histogram()
ggplot(train, aes(x = R_moment_2)) + geom_histogram()
ggplot(train, aes(x = R_moment_3)) + geom_histogram()
ggplot(train, aes(x = R_moment_4)) + geom_histogram()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_1
ggplot(train, aes(x = Re, y = R_moment_1)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_1)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_1)) + geom_point()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_2
ggplot(train, aes(x = Re, y = R_moment_2)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_2)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_2)) + geom_point()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_3
ggplot(train, aes(x = Re, y = R_moment_3)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_3)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_3)) + geom_point()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_4
ggplot(train, aes(x = Re, y = R_moment_4)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_4)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_4)) + geom_point()
```

The plot below suggests a very possible interaction effect between Stokes number and Reynolds number on 1st Moment:

```{r}
ggplot(data = train, aes(y = R_moment_1, x = St, color = flow, shape = Re.fac)) + 
  geom_point() +
  labs(x = 'St: Stokes Number',
       y = '1st Moment',
       title = 'First moment vs. Stokes Number',
       subtitle = 'faceted by Reynolds Number Level and Flow Level' )
```

# Simple Linear Regression

-   We made `Fr` a categorical variable when fitting a linear regression model, as `Fr` only has three unique values both in the training and testing dataset; one of these values is `Inf`, which should not be used in a linear regression analysis. 

#### Data Wrangling

#### First Moment Linear Fit

```{r}
#Not sure why NAs are generated for Re.facmid:flowsupercritical int term

lm.fit.m1 <- lm(R_moment_1 ~ Re.fac + St + flow + Re.fac:flow, data = train)
summary(lm.fit.m1)
```

Using 5-fold cross-validation to estimate the test set error

```{r}
set.seed(123) 
train.control <- trainControl(method = "cv", number = 5)
# Train the model
model <- train(R_moment_1 ~ Re.fac + poly(St,2) + flow + Re.fac:flow, data = train, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

Trying using polynomial terms up to degree of 5 for stokes number:

```{r}
get_poly <- function(response){
  fit.1 <- lm(response ~ St + flow + Re.fac, data=train)
  fit.2 <- lm(response ~ poly(St, 2) + flow + Re.fac, data=train)
  fit.3 <- lm(response ~ poly(St, 3) + flow + Re.fac, data=train)
  fit.4 <- lm(response ~ poly(St, 4) + flow + Re.fac, data=train)
  fit.5 <- lm(response ~ poly(St, 5) + flow + Re.fac, data=train)
  return(anova(fit.1,fit.2,fit.3,fit.4,fit.5))
}

moment1 <- get_poly(train$R_moment_1) 
print(moment1)
```

Judging from the p value for the associated F-statistics, only the first order term is necessary.

#### Moments 2-4:

```{r}
lm.fit.m2 <- lm(log(R_moment_2) ~ Re + St + flow, data = train)
summary(lm.fit.m2)

lm.fit.m3 <- lm(log(R_moment_3) ~ Re + St + flow, data = train)
summary(lm.fit.m3)

lm.fit.m4 <- lm(log(R_moment_4) ~ Re + St + flow, data = train)
summary(lm.fit.m4)
```

Considering a simple linear regression on the first moment: we have a 0.94 adjusted R squared value and non significant F-statistics; however the residual vs fitted values plot indicates a obvious non-linear trend, which suggests that the linearity assumption is violated.

# Ridge Regression

```{r}
library(glmnet)

get_ridge_MSE<- function(arg){
  
  train <- read.csv("data-train.csv")
  # test <- as.matrix(test)
  
  ridge.train.idx <- sample(1:nrow(train), nrow(train)*0.75)
  
  if(arg == 1){
    response = train$R_moment_1
    ridge.train <- subset(train[ridge.train.idx, ], select = c(St, Re, Fr, R_moment_1))
    ridge.ytest <- response[- ridge.train.idx]
    x <- model.matrix(R_moment_1~.,ridge.train)[,-1]
    y <- ridge.train$R_moment_1
  } else if(arg == 2){
    response = train$R_moment_2
    ridge.train <- subset(train[ridge.train.idx, ], select = c(St, Re, Fr, R_moment_2))
    ridge.ytest <- response[- ridge.train.idx]
    x <- model.matrix(R_moment_2~.,ridge.train)[,-1]
    y <- ridge.train$R_moment_2
  }else if(arg == 3){
    response = train$R_moment_3
    ridge.train <- subset(train[ridge.train.idx, ], select = c(St, Re, Fr, R_moment_3))
    ridge.ytest <- train$response[- ridge.train.idx]
    x <- model.matrix(R_moment_3~.,ridge.train)[,-1]
    y <- ridge.train$R_moment_3
  }else{
    response = train$R_moment_4
    ridge.train <- subset(train[ridge.train.idx, ], select = c(St, Re, Fr, R_moment_4))
    ridge.ytest <- train$response[- ridge.train.idx]
    x <- model.matrix(R_moment_4~.,ridge.train)[,-1]
    y <- ridge.train$R_moment_4
  }
  
  ridge.test <- as.matrix(subset(train[-ridge.train.idx, ], select = c(St, Re, Fr)))
  x <- subset(x, select = c(St, Re, Fr))
  
  
  grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
  cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = grid)
  opt_lambda <- cv_fit$lambda.min
  
  ridge.mod <- glmnet(x, y, alpha = 0, lambda = opt_lambda)
  ridge.pred <- predict(ridge.mod, s = 4, newx = ridge.test)
  MSE <- mean((ridge.pred - ridge.ytest)^2) # calculate MSE
  return(MSE)
}

```

```{r}

MSE <- c()
col <- c("R_moment_1", "R_moment_2", "R_moment_3", "R_moment_4")

for(i in 1:4){
  val <- get_ridge_MSE(1)
  MSE <- c(MSE, val)
}

MSE_df <- data.frame(col, MSE)
```

