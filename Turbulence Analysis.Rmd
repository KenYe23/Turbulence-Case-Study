---
title: "Turbulence Analysis"
author: "Tingnan Hu, Peter Liu, Islina Shan, Ken Ye"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Load Data & Libraries

```{r}
train <- read.csv("data-train.csv")
test <- read.csv("data-test.csv")
library(ggplot2)
```

```{r}
head(train)
```

# EDA

```{r}
summary(train)
```

```{r}
# Create histograms for the predictor variables
ggplot(train, aes(x = Re)) + geom_histogram()
ggplot(train, aes(x = Fr)) + geom_histogram()
ggplot(train, aes(x = St)) + geom_histogram()
ggplot(train, aes(x = R_moment_1)) + geom_histogram()
ggplot(train, aes(x = R_moment_2)) + geom_histogram()
ggplot(train, aes(x = R_moment_3)) + geom_histogram()
ggplot(train, aes(x = R_moment_4)) + geom_histogram()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_1
ggplot(train, aes(x = Re, y = R_moment_1)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_1)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_1)) + geom_point()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_2
ggplot(train, aes(x = Re, y = R_moment_2)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_2)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_2)) + geom_point()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_3
ggplot(train, aes(x = Re, y = R_moment_3)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_3)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_3)) + geom_point()
```

```{r}
# Create scatterplots to explore relationships between predictor variables with R_moment_4
ggplot(train, aes(x = Re, y = R_moment_4)) + geom_point()
ggplot(train, aes(x = Fr, y = R_moment_4)) + geom_point()
ggplot(train, aes(x = St, y = R_moment_4)) + geom_point()
```

The plot below suggests a very possible interaction effect between Stokes number and Reynolds number on 1st Moment:

```{r}
ggplot(data = train, aes(y = R_moment_1, x = St, color = flow, shape = Re.fac)) + 
  geom_point() +
  labs(x = 'St: Stokes Number',
       y = '1st Moment',
       title = 'First moment vs. Stokes Number',
       subtitle = 'faceted by Reynolds Number Level and Flow Level' )
```

# Simple Linear Regression

-   Justify making Fr as a categorical variable

#### Data Wrangling

```{r}
library(dplyr)
library(car)

train <- train %>% 
  mutate(flow = case_when(
    Fr < 0.1 ~ 'super subcritical',
    Fr < 1 & Fr > 0.1 ~ 'subcritical',
    Fr > 1 ~ 'supercritical'
  )) %>% 
  mutate(
    log.moment.2 <- log(R_moment_2),
    log.moment.3 <- log(R_moment_3),
    log.moment.4 <- log(R_moment_4)
  ) %>% 
  mutate(
    Re.fac = case_when(
      Re == 90 ~ 'low',
      Re == 224 ~ 'mid',
      Re == 398 ~ 'high'
    )
  )

train %>% 
  filter(Re == 224 & Fr == Inf)
```

#### First Moment Linear Fit

```{r}
#Not sure why NAs are generated for Re.facmid:flowsupercritical int term

lm.fit.m1 <- lm(R_moment_1 ~ Re.fac + St + flow + Re.fac:flow, data = train)
summary(lm.fit.m1)
```

Using 5-fold cross-validation to estimate the test set error

```{r}
set.seed(123) 
train.control <- trainControl(method = "cv", number = 5)
# Train the model
model <- train(R_moment_1 ~ Re.fac + poly(St,2) + flow + Re.fac:flow, data = train, method = "lm",
               trControl = train.control)
# Summarize the results
print(model)
```

Trying using polynomial terms up to degree of 5 for stokes number:

```{r}
get_poly <- function(response){
  fit.1 <- lm(response ~ St + flow + Re.fac, data=train)
  fit.2 <- lm(response ~ poly(St, 2) + flow + Re.fac, data=train)
  fit.3 <- lm(response ~ poly(St, 3) + flow + Re.fac, data=train)
  fit.4 <- lm(response ~ poly(St, 4) + flow + Re.fac, data=train)
  fit.5 <- lm(response ~ poly(St, 5) + flow + Re.fac, data=train)
  return(anova(fit.1,fit.2,fit.3,fit.4,fit.5))
}

moment1 <- get_poly(train$R_moment_1) 
print(moment1)
```

Judging from the p value for the associated F-statistics, only the first order term is necessary.

#### Moments 2-4:

```{r}
lm.fit.m2 <- lm(log(R_moment_2) ~ Re + St + flow, data = train)
summary(lm.fit.m2)

lm.fit.m3 <- lm(log(R_moment_3) ~ Re + St + flow, data = train)
summary(lm.fit.m3)

lm.fit.m4 <- lm(log(R_moment_4) ~ Re + St + flow, data = train)
summary(lm.fit.m4)
```

Considering a simple linear regression on the first moment: we have a 0.94 adjusted R squared value and non significant F-statistics; however the residual vs fitted values plot indicates a obvious non-linear trend, which suggests that the linearity assumption is violated.

# Ridge Regression

```{r}
library(glmnet)

train <- read.csv("data-train.csv")
test <- as.matrix(test)
x <- model.matrix(R_moment_1~.,train)[,-1]
x <- subset(x, select = c(St, Re, Fr))
y <- train$R_moment_1

grid <- 10^seq(10, -2, length = 100) # grid of values for lambda param
cv_fit <- cv.glmnet(x, y, alpha = 0, lambda = grid)
opt_lambda <- cv_fit$lambda.min

ridge.mod <- glmnet(x, y, alpha = 0, lambda = opt_lambda)
ridge.pred <- predict(ridge.mod, s = 4, newx = test)

```
